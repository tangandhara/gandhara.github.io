{"title":"Time Series Analysis","markdown":{"yaml":{"title":"Time Series Analysis","author":"Tan Gandhara","categories":["time series","forecasting","analysis"],"image":"revolut-logo-min.jpg","subtitle":"A time series analysis of Revolut spending data","format":"html","editor":"visual"},"headingText":"Revolut Spending Data","containsRefs":false,"markdown":"\n\n\n```{r}\n#| label: load-packages\n#| include: false\n \nlibrary(fpp3)\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(readxl)\nlibrary(httr)\nlibrary(knitr)\nlibrary(prophet)\n\nrevolut_spending <- read_excel(\"C:/Users/Tan/Downloads/revolutspendingondebitcardsdataset230323.xlsx\", \n                               sheet = \"Spending by sector\", col_types = c(\"date\", \n                                                                           \"numeric\", \"numeric\", \"numeric\", \n                                                                           \"numeric\", \"numeric\", \"numeric\", \n                                                                           \"numeric\"), skip = 2)\n\nrs3 <- revolut_spending[-c(3:8)]\n\n\nrs3$Date <- as.Date(rs3$Date)\nrs3 <- as_tsibble(rs3, index = Date)\n\n```\n\nThis time series analysis focuses on [weekly data](https://www.ons.gov.uk/economy/economicoutputandproductivity/output/datasets/revolutspendingondebitcards \"Revolut spending on debit cards\") available from the Office for National Statistics (ONS). Revolut is one of a number of digital banks that have emerged in recent years and it has around 4.8 million users within the UK financial payment ecosystem. The bank has been providing data to the ONS since the start of the COVID-19 pandemic and the ONS indexes data at the average February 2020 spending level as a pre-pandemic baseline. They also apply a 7-day rolling average to the data to take into account any intra-week spending cyclicality.\n\nWhile this data is useful for analysing spending changes since 2020 it is important to note a number of limitations:\n\n-   Revolut customers tend to be younger and more metropolitan than the average UK consumer, so spending may not be representative of the overall UK macroeconomic picture.\n\n-   The indices in the dataset do not take into account inflation and are presented on a nominal basis and are not adjusted for price increases over time. According to the ONS the CPI rate was at 1.8% in January 2020 but by January 2023 was at 10.1%.\n\n-   Within the UK financial transaction ecosystem there has been a shift away from cash as a payment medium in favour of card spending. This results in indices being uplifted over time in areas where consumers replace low value cash transactions with low value card transactions instead. This is more likely to be true in this dataset given the demographic profile of Revolut customers.\n\nThe full background on methodology and limitations is available from the ONS [here](https://www.ons.gov.uk/economy/economicoutputandproductivity/output/methodologies/usingrevolutcardholderdatatoderiverealtimeindicatorsofconsumerspendingqmi \"Using Revolut cardholder data to derive real-time indicators of consumer spending QMI\").\n\n## Time Series Analysis\n\nThis project utilizes the indexed rolling 7-day average total spend value from the spending by sector data to identify trends and forecast 28 days ahead. For the analysis, I will use the following methods: exponential smoothing, ARIMA, and forecasting using Prophet.\n\nAfter some initial cleaning of dataset to ensure it is in the correct format to conduct time series analysis in R, figure 1 shows plot of the spending over time shows how these levels changed over the course of the pandemic.\n\n```{r}\n#| echo: false\n\nrs3 |> ggplot(aes(y = Total, x = Date)) +\n                  geom_line()+\n  labs(y = \"Spend (£)\",\n         title = \"Revolut debit card spening 7-day rolling average\")+\n   geom_hline(aes(yintercept = 100), linetype = 3)+\n  annotate(\"text\", x = rs3$Date[1000], y = 103, label=\"Index: 100 = February 2020 mean value\", size = 2)+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\nIn general, what we can see is a clear upward trend across the three years and some seasonality exhibited around August and December-January. While the initial lockdown might be easily identifiable by the sharp dip during Q1 2020, it may be helpful to provide a reminder of all the restrictions implemented during the pandemic, and the plot below adds further context.\n\nThe red lines are indicative of the main lockdowns and the restrictions imposed due to the emergence of the Omicron variant in late 2021. Although there were many stages of 'unlocking' of restrictions, I have chosen to highlight two key moments with green lines.\n\n```{r}\n#| echo: false\n\nrs3 |> ggplot(aes(y = Total, x = Date)) +\n                  geom_line()+\n                  labs(y = \"Spend (£)\",\n                       title = \"Revolut debit card spening with COVID-19 restrictions\")+\n    geom_hline(aes(yintercept = 100), linetype = 3)+\n    geom_vline(aes(xintercept = Date[86]),\n               linetype = 4, colour = \"#EA526F\", linewidth = 1)+\n    geom_vline(aes(xintercept = Date[310]),\n               linetype = 4, colour = \"#EA526F\", linewidth = 1)+\n    geom_vline(aes(xintercept = Date[370]),\n               linetype = 4, colour = \"#EA526F\", linewidth = 1)+\n    geom_vline(aes(xintercept = Date[708]),\n               linetype = 4, colour = \"#EA526F\", linewidth = 1)+\n    geom_vline(aes(xintercept = Date[216]),\n               linetype =2, colour = \"#49DCB1\", linewidth = 1)+\n    geom_vline(aes(xintercept = Date[786]),\n               linetype = 2, colour = \"#49DCB1\", linewidth = 1)+\n    annotate(\"text\", x = rs3$Date[770], y = 68, label=\"Living with Covid Plan implemented\", angle=90, size = 2)+\n    annotate(\"text\", x = rs3$Date[200], y = 120, label=\"Eat Out To Help Out\", angle=90, size = 2)+\n    annotate(\"text\", x = rs3$Date[70], y = 120, label=\"Lockdown #1\", angle=90, size = 2)+\n    annotate(\"text\", x = rs3$Date[295], y = 120, label=\"Lockdown #2\", angle=90, size = 2)+\n    annotate(\"text\", x = rs3$Date[355], y = 120, label=\"Lockdown #3\", angle=90, size = 2)+\n    annotate(\"text\", x = rs3$Date[690], y = 70, label=\"Omicron restrictions\", angle=90, size = 2)+\n    theme_bw()+\n    theme(plot.title = element_text(face=\"bold\"))\n\n```\n\nThe first of these easing periods is the Eat Out To Help Out programme during August 2020 which offered discounted meals in pubs, restaurants, and other hospitality outlets to support the sector. The second is in February 2022 when official COVID-19 restrictions were ended under the Living With Covid Plan. As a result of these two interventions, we can clearly identify periods of sustained spending as people had greater freedom of movement in their daily lives.\n\nIn spite of this, plotting the years against each reveals patterns in the second half of the year, particularly through Q4, that are broadly similar.\n\n```{r}\n#| echo: false\n\nrs3 |> gg_season(Total, labels = \"both\")+\nlabs(y = \"Spend (£)\",\n     title = \"Revolut debit card spending by year\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\n### Exponential Smoothing\n\n#### Holt's method with trend\n\nFor this dataset I have chosen to experiment with the exponential smoothing method with trend and then later add in the seasonality component to determine if this improves the forecast.\n\n```{r}\n#| echo: true\n\nfit <- rs3 |>\n  model(\n    `Holt's method` = ETS(Total ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )\nfabletools::report(fit)\n```\n\nThis trended exponential smoothing model has a very high alpha value indicating that values in the past are given less weight than more recent values due to the exponential decaying built into the model. In addition, the beta value is high and this takes into changes in the data.\n\nPlotting the forecast of the next 28 days sees quite a steady decline in the mean value and quite a wide distribution across both the 80% and 95% intervals, with the lower and upper bounds of the 95% interval ranging below 0 and close to 300.\n\n```{r}\n#| echo: false\n\nfc <- fit |> forecast(h = 28)\nfc |> autoplot(rs3)+\nlabs(y = \"Spend (£)\",\n     title = \"Exponential smoothing: Holt's method trend\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\nGiven the macroeconomic conditions in the British economy are have not been particularly positive, this decline might not seem unreasonable and the model may benefit from the introduction of some level of dampening so as not to over-forecast.\n\n```{r}\n#| echo: true\n\nfit2 <- rs3 |>\n    model(`Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n    )\nfabletools::report(fit2)\n```\n\nThe model estimates phi, the dampening coefficient, to be 0.80 and we see a slight decrease in the alpha and increase beta values. This dampened model also performs better compared top the previous model when we compare AIC, AICc, and BIC values. The table below compares the two models and we can see that the damped Holt's method has a lower RMSE so makes it a better choice.\n\n```{r}\n#| echo: true\n\nrs3 |> \n    model(\n        `Holt's method` = ETS(Total ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n        `Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n        ) |> accuracy()\n```\n\nThe forecasts from the damped model also exhibit less variability in the 80% and 95% intervals than the earlier model.\n\n```{r}\n#| echo: false\n\n\nfc2 <- fit2 |> forecast(h = 28)\nfc2 |> autoplot(rs3) +\nlabs(y = \"Spend (£)\",\n     title = \"Exponential smoothing: Damped Holt's method trend\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\nWhen comparing the two forecasts, the dampened forecast, unsurprisingly, decreases less rapidly or as extreme at the median value.\n\n```{r}\n#| echo: false\n\nrs3 |> \n    model(\n        `Holt's method` = ETS(Total ~ error(\"A\") +\n                                  trend(\"A\") + season(\"N\")),\n        `Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n    ) |>\n    forecast(h = 15) |> \n    autoplot(rs3, level = NULL) +\n    labs(title = \"Revolut debit card spending with exponential smoothing forecasts\",\n         y = \"Spend (£)\") +\n    guides(colour = guide_legend(title = \"Forecast method\"))+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\n### Holt-Winters' method with seasonality\n\nTo add in a seasonality component to the model we can use the Holt-Winters' method either with an additive seasonality component, which assumes the seasonal variations within the time series to be approximately constant, or multiplicative seasonality component, where the assumption is that the variations are proportional to the level of the series.\n\nWith the initial modelling, we can see that the additive model performs better with its lower AIC, AICc, and BIC values, in addition to lower MSE and AMSE values.\n\n```{r}\n#| echo: false\n#| warning: false\n\n\nfit3 <- rs3 |> \n    model(\n        Additive = ETS(Total ~ error(\"A\") + trend(\"A\") + season(\"A\")),\n        Multiplicative = ETS(Total ~ error(\"M\") + trend(\"A\") + season(\"M\"))\n    )\nfabletools::report(fit3)\n```\n\nThe difference between the two models is evident when forecasting the next 28 days, with the mean values for additive model increasing while the multiplicative model shows a decline.\n\n```{r}\n#| echo: false\n#| warning: false\n\n\nfc3 <- fit3 |> forecast(h = 28)\nfc3 |> autoplot(rs3, facets = TRUE)+\n    labs(title = \"Exponential smoothing: Holt-Winters' method with seasonality\",\n         y = \"Spend (£)\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\nSimilarly, the RMSE for the additive model is better than multiplicative one, although it does not perform as well as the RMSE for the damped Holt's method above.\n\n```{r}\n#| echo: false\n#| warning: false\n\n\nfit3 |> accuracy()\n```\n\nHowever, it is possible to combine the dampened trend method with both additive and multiplicative seasonality. Both models see a drop in MSE and the AIC, AICc, and BIC values fall significantly, with the dampened multiplicative Holt Winter's method having very similar values to the dampened Holt's method. The RMSE value improves but not to the extent that either model match the model using damped Holt's method.\n\n```{r}\n#| echo: false\n#| warning: false\n\n\nfit4 <- rs3 |> \n    model(\n        Additive = ETS(Total ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\n        Multiplicative = ETS(Total ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n    )\nfabletools::report(fit4)\n```\n\n```{r}\n#| echo: false\n#| warning: false\n\n\nfit4 |> accuracy()\n```\n\nThe plot here of the 28 day ahead forecast exhibits shallower increases for both models and a narrower set of intervals at 80% and 95%.\n\n```{r}\n#| echo: false\n#| warning: false\n\nfc4 <- fit4 |> forecast(h = 28)\nfc4 |> autoplot(rs3)+theme_bw()+\n    labs(title = \"Damped exponential smoothing: Holt-Winters'\\nmethod with seasonality\",\n         y = \"Spend (£)\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n\n```\n\n#### Model selection\n\nHaving previously worked through a number of combinations of exponential smoothing models, both with and without dampening or seasonality, it seems that the damped Holt's method without seasonality performed best when comparing AICc and RMSE values. However, it's possible to employ the ETS() function in R to generate a model that minimises the AICc value.\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3 |> \n    model(\n        `R Preferred` = ETS(Total)\n    ) |> report()\n```\n\nComparisons between this preferred model and the damped Holt's model from earlier are below:\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3 |> \n    model(\n        `R Preferred` = ETS(Total),\n        `Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n    ) |> report()\n```\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3 |> \n    model(\n        `R Preferred` = ETS(Total),\n        `Damped Holt's method` = ETS(Total ~ error(\"A\") +\n                                         trend(\"Ad\") + season(\"N\"))\n    ) |> accuracy()\n```\n\nThe output above tells us the model preferred by R uses Holt's method where the error is multiplicative (rather than additive as has been used in the examples above), with an additive trend that is dampened. The value for alpha is very high, reflecting how the weight of the past observations decays quite rapidly. The AICc value is the lowest of all the models we have tested above.\n\nThe forecast of the next 28 days reflects the marginal decline exhibited in some of the other dampened models above. Given that the alpha value is relatively high it is not unsurprising that this model forecasts the mean value to be close to the total 7-day average spend in 2023.\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3 |> model(`R Preferred` = ETS(Total)) |> forecast(h = 28) |> autoplot(rs3) +\n    labs(title = \"R's preferred model: ETS(M,Ad,N)\",\n         y = \"Spend (£)\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n\n```\n\nWe can use cross-validation based on a rolling forecasting origin, starting at the end of 2022 (chosen due to the impact of the pandemic prior to that) and increasing by one step each time. The result of this process is that the model preferred by R performs the best both when comparing AICc and RMSE.\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs_fit <- rs3 |> model(`R Preferred` = ETS(Total),\n                       `Damped Holt's method` = ETS(Total ~ error(\"A\") + trend(\"Ad\") + season(\"N\"))) |> report()\nrs_fit\n```\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3_tr <- rs3 |> \n    stretch_tsibble(.init = 1096, .step = 1)\nrs3_tr |>\n    model(`R Preferred` = ETS(Total),\n          `Damped Holt's method` = ETS(Total ~ error(\"A\") + trend(\"Ad\") + season(\"N\")))|>\n              forecast(h = 28) |>\n              accuracy(rs3)\n```\n\nChecking the residuals of R's preferred model, we can see that the innovation residuals appear to have a constant variance and mean of zero. The histogram exhibits some degree of normality although the peak is a little high. The ACF plot have a number of significant that decay exponentially.\n\n```{r}\n#| echo: false\n#| warning: false\n\nbest_fit <- rs3 |> model(ETS(Total ~ error(\"M\") + trend(\"Ad\") + season(\"N\")))\ngg_tsresiduals(best_fit)+\n  ggtitle(\"Residuals of the R's preferred model\")\n```\n\nIn contrast to the damped Holt's method model, while the residuals have a mean of zero, the variance does appear to increase as we reach the end of 2022 and start 2023. The histogram is slightly left-skewed and peaks so may also fail the normality test. The ACF plot bears some similarity to R's preferred model with a number of significant lags and before decaying. Given this, R's preferred model seems to be the better model.\n\n```{r}\n#| echo: false\n#| warning: false\n\nbest_damped <- rs3 |> model(`Damped Holt's method` = ETS(Total ~ error(\"A\") + trend(\"Ad\") + season(\"N\")))\ngg_tsresiduals(best_damped)+\n  ggtitle(\"Residuals of the damped Holt's method model\")\n```\n\n### ARIMA modelling\n\nLooking at the initial plot of data above, it is clear that the data is not stationary and the ACF plot below shows the same data when it is not differenced up to lag 100. The lags remain significant are taking a long time to decay because each is correlated to the previous one. This makes sense when we consider that each daily value in the dataset relates to rolling 7-day average spend so we would expect correlation between the current and previous values.\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3 |> ACF(Total, lag_max = 100) |>\n    autoplot() + labs(subtitle = \"Revolut spending 7-day rolling average\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\nTo make the data stationary, I have had to apply a log transformation to stabilise the variance and first order differencing to stabilise the mean. The resulting plot has a number of significant spikes around the time of lockdown restrictions being introduced, but overall resembles white noise.\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3 |>\n    ggplot(aes(x = Date, y =  difference(log(Total))))+\n    geom_line()+ \n    labs(title = \"Changes in Revolut spending 7-day rolling average - differenced\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n\n```\n\nThe differenced ACF plot exhibits significant lags up to lag 6 before decaying away.\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3 |> ACF(difference(log(Total))) |>\n    autoplot() + labs(subtitle = \"Changes in Revolut spending 7-day rolling average - differenced\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n\n```\n\nIt is also possible to confirm stationarity of this differenced data with a KPSS unit root test, which gives a small test statistic and a p-value of 0.1 and allows us to assume the data is stationary:\n\n```{r}\n#| echo: false\n#| warning: false\n\n(rs3 |> mutate(\n    diff_total = difference(log(Total))\n) |> features(diff_total, unitroot_kpss))\n```\n\nThe time plot and ACF & PACF plots of the stationary data are show below.\n\n```{r}\n#| echo: false\n#| warning: false\n\nrs3 |> gg_tsdisplay(difference(log(Total)),lag_max = 100, plot_type = \"partial\")+\n  ggtitle(\"Log transformed and differenced spending data\")\n```\n\nGiven the data is a 7-day rolling average and that the ACF plot appears to show a sinusodial and seasonal pattern of aproximately 7 days, a seasonal ARIMA model would be appropriate. In order to achieve this, I will let R try to find the best model order. The first is the default stepwise procedure and the second one works harder to search for a better model.\n\n```{r}\n#| echo: true\n#| warning: false\n\nar_fit <- rs3 |> model(stepwise = ARIMA(Total),\n             search = ARIMA(Total, stepwise=FALSE, approx = FALSE))\n```\n\nAs we can from the output here, R has ARIMA(3,1,0)(1,0,1)~7~ for both models\n\n```{r}\n#| echo: false\n#| warning: false\n\nar_fit |> pivot_longer(everything(), names_to = \"Model name\",\n                    values_to = \"Orders\")\n```\n\n```{r}\n#| echo: false\n#| warning: false\n\nglance(ar_fit) |> arrange(AICc) |> select(.model:BIC)\n```\n\nThe residuals, as shown below, appear to have a constant mean and variance in the time plot and the ACF, whilst show some spikes appears consistent with white noise. However, the model fails the Ljung-Box text for white noise. This model is still left-skewed and peaks a little too high in the histogram, thus failing the normality test.\n\n```{r}\n#| echo: false\n#| warning: false\n\nar_fit |> select(search) |> gg_tsresiduals(lag=100)+\n  ggtitle(\"Residuals of ARIMA(3,1,0)(1,0,1)7 model\")\n```\n\n```{r}\n#| echo: false\n#| warning: false\n\naugment(ar_fit) |>\n    filter(.model == \"search\") |>\n    features(.innov, ljung_box, lag=100, dof=7)\n```\n\nAlthough this model does not pass all of the residual tests we can still use it to forecast, bearing in mind the limitations concerning the accuracy of prediction intervals. Forecasts for the next 28 days are shown below.\n\n```{r}\n#| echo: false\n#| warning: false\n\nforecast(ar_fit, h=28) |>\n    filter(.model==\"search\") |>\n    autoplot(rs3) +\n    labs(title = \"ARIMA(3,1,0)(1,0,1)7 model\",\n         y=\"Spend (£)\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\n### Prophet\n\nFor an alternative approach, I have chosen to use [Meta's Prophet tool](https://facebook.github.io/prophet/) for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.\n\nIn order to use package, the dataset had to be modified so the date variable was renamed 'ds' and the total variable as 'y'. In addition, Prophet allows for custom holidays to be added to the model to take into account national holidays that occur. The holidays function can also be used to deal with systemic shocks that would impact a time series so as to prevent the trend component capturing any peaks or troughs in the data. As such, I have created an additional tibble that holds the dates for the three UK lockdowns and the period of restrictions in place following the emergence of the omicron variant of COVID-19.\n\n```{r}\n#| echo: false\n#| warning: false\n\ndf <- rs3 |> rename(\n    ds = \"Date\",\n    y = \"Total\"\n)\nhead(df,5)\n\n```\n\n```{r}\n#| echo: false\n#| warning: false\n\nlockdowns <- tibble(\nholiday = c('lockdown_1', 'lockdown_2', 'lockdown_3', 'omicron'),\nds = as.Date(c('2020-03-26', '2020-11-05', '2021-01-05', '2021-12-08')),\nlower_window = 0,\nds_upper = as.Date(c('2020-05-11', '2020-12-02', '2021-05-17', '2022-02-24')),\nupper_window = 0)\nlockdowns\n```\n\nWith the data set up to, I started by modelling the time series with limited changes to the available parameters. However, the *changepoint_prior_scale* parameter was increased to make the trend more flexible and the seasonality mode changed from additive to multiplicative.\n\n```{r}\n#| echo: true\n#| warning: false\n\nm <- prophet(df, changepoint.prior.scale = 0.3, holidays = lockdowns, seasonality.mode = \"multiplicative\")\n\nfuture <- make_future_dataframe(m, periods = 28)\n\nforecast <- predict(m, future)\n\nplot(m, forecast)+\n    labs(title = \"Prophet multiplicative model\",\n         y=\"Spend (£)\",\n        x=\"Date\")+\n  theme_bw()+\n  theme(plot.title = element_text(face=\"bold\"))\n```\n\nThis interactive plot allows for the model to be viewed in more detail:\n\n```{r}\n#| echo: false\n#| warning: false\n\ndyplot.prophet(m, forecast)\n```\n\nUsing Prophet we are able to decompose the model and look at each component separately.\n\n```{r}\n#| echo: false\n#| warning: false\n\n\nprophet_plot_components(m, forecast)\n```\n\nFinally, using cross-validation we can measure forecast error against historical data in a method akin to a rolling forecast origin used earlier. With Prophet I have chosen to select an initial period of 400 days (i.e. up to early February 2021) and made predictions every 90 days for a forecast horizon of 28 days. Based on the current time series, this corresponded to 9 forecasts.\n\n```{r}\n#| echo: true\n#| warning: false\n\n\ndf.cv <- cross_validation(m, initial = 400, period = 90, horizon = 28, units = 'days')\n```\n\nOnce computed, we can visualise a range of statistics of prediction performance. In the plot below, the dots show the absolute percentage error and the blue line the mean absolute percentage error over the forecast horizon. Forecast error here remains up to 10% for the first 12 days but then steadily increases to a maximum of around 15% for predictions 28 days out.\n\n```{r}\n#| echo: false\n#| warning: false\n\n\ndf.p <- performance_metrics(df.cv)\nhead(df.p)\n```\n\n```{r}\n#| echo: false\n#| warning: false\n\nplot_cross_validation_metric(df.cv, metric = 'mape')\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"visual","theme":"journal","title-block-banner":true,"title":"Time Series Analysis","author":"Tan Gandhara","categories":["time series","forecasting","analysis"],"image":"revolut-logo-min.jpg","subtitle":"A time series analysis of Revolut spending data"},"extensions":{"book":{"multiFile":true}}}}}